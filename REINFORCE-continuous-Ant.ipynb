{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "# setting manual seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from parallelEnv import parallelEnv\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# imports for rendering outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(8,) ... State space: Box(111,)\n",
      "action_high: [1. 1. 1. 1. 1. 1. 1. 1.]\t action_low: [-1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# lets set up the cartpole environment\n",
    "env = gym.make('Ant-v2')\n",
    "env.seed(0)\n",
    "\n",
    "# lets find about the action and state space\n",
    "print(\"Action space: {} ... State space: {}\".format(env.action_space,env.observation_space))\n",
    "\n",
    "# lets print some info about action space range\n",
    "action_high = env.action_space.high\n",
    "action_low = env.action_space.low\n",
    "print(\"action_high: {}\\t action_low: {}\".format(action_high,action_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets play a random episode\n",
    "\n",
    "# reset env\n",
    "state = env.reset()\n",
    "step_count = 0\n",
    "done = False\n",
    "\n",
    "while (not done):\n",
    "    # random action\n",
    "    action = env.action_space.sample()\n",
    "    # taking step in env\n",
    "    next_state,reward,done,_ = env.step(action)\n",
    "    \n",
    "    # displaying progress\n",
    "    print (\"Step : {} ... state: {} ... action: {} ... reward: {} ... done: {}\".format(step_count,\n",
    "                                                                                       state[:4],action[:4],reward,done))\n",
    "    \n",
    "    # updating state and step count\n",
    "    state = next_state\n",
    "    step_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"using\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_size,action_size,action_high,action_low,hidden_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        # action range\n",
    "        self.action_high = torch.tensor(action_high).to(device)\n",
    "        self.action_low = torch.tensor(action_low).to(device)\n",
    "        \n",
    "        # fc layers for the policy network\n",
    "        self.fc1 = nn.Linear(state_size,512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3_action = nn.Linear(128,action_size)\n",
    "        self.fc3_std = nn.Linear(128,action_size)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        net = F.relu(self.fc1(state))\n",
    "        net = F.relu(self.fc2(net))\n",
    "        action_mean = F.sigmoid(self.fc3_action(net))\n",
    "        # rescale action mean\n",
    "        action_mean_ = (self.action_high-self.action_low)*action_mean + self.action_low\n",
    "        action_std = F.sigmoid(self.fc3_std(net))\n",
    "        return action_mean_,action_std\n",
    "    \n",
    "    def act(self,state):\n",
    "        # converting state from numpy array to pytorch tensor on the \"device\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_mean,action_std = self.forward(state)\n",
    "        prob_dist = Normal(action_mean,action_std)\n",
    "        action = prob_dist.sample()\n",
    "        log_prob = prob_dist.log_prob(action)\n",
    "        return action.cpu().numpy(),torch.sum(log_prob,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import accumulate\n",
    "\n",
    "def compute_future_rewards(rewards,gamma):\n",
    "    future_rewards = np.zeros_like(rewards)\n",
    "    discounted_rewards = np.zeros(rewards.shape[0])\n",
    "    \n",
    "    for time_step in range(future_rewards.shape[1]-1,-1,-1):\n",
    "        future_rewards[:,time_step] = rewards[:,time_step] + gamma*discounted_rewards\n",
    "        discounted_rewards = future_rewards[:,time_step]\n",
    "    return future_rewards\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self,env_name,learning_rate=1e-3):\n",
    "        self.env = parallelEnv(env_name=env_name,n=8,seed=0)\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.shape[0]\n",
    "        self.policy = Policy(state_size=self.nS,hidden_size=128,action_size=self.nA,\n",
    "                             action_low=action_low,action_high=action_high).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def train(self,max_opt_steps=1000,num_trajectories=8,horizon=1000,gamma=.99,PRINT_EVERY=100):\n",
    "        # store eps scores\n",
    "        scores = []\n",
    "        scores_window = deque(maxlen=100)\n",
    "        \n",
    "        for opt_step in range(1,max_opt_steps+1):\n",
    "            rewards = np.zeros([num_trajectories,horizon])\n",
    "            log_probs = torch.zeros([num_trajectories,horizon])\n",
    "            \n",
    "            for traj_count in range(1):\n",
    "                # reset state\n",
    "                state = self.env.reset()\n",
    "            \n",
    "                # play an episode\n",
    "                for t in range(horizon): \n",
    "                    action,log_prob = self.policy.act(state)\n",
    "                    next_state,reward,done,_ = self.env.step(action)\n",
    "\n",
    "                    # update state\n",
    "                    state = next_state\n",
    "                    log_probs[:,t] = log_prob\n",
    "                    rewards[:,t] = reward\n",
    "\n",
    "                    # break if done\n",
    "                    if np.any(done):\n",
    "                        break\n",
    "            \n",
    "            # compute advantage estimate to reduce variance\n",
    "            future_rewards = compute_future_rewards(rewards,gamma)\n",
    "            b = future_rewards.mean(axis=0)\n",
    "            A = (future_rewards - b)/future_rewards.std(axis=0)\n",
    "            A = torch.from_numpy(A).double().to(device)\n",
    "            \n",
    "            log_probs = log_probs.double().to(device)\n",
    "            # compute loss and applying gradient\n",
    "            loss = torch.sum(-log_probs*A)/(num_trajectories*horizon)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            trajectory_total_rewards = rewards.sum(axis=1)\n",
    "            # update scores and score_window\n",
    "            scores.append(np.mean(trajectory_total_rewards))\n",
    "            scores_window.append(np.mean(trajectory_total_rewards))\n",
    "            \n",
    "            #printing progress\n",
    "            if opt_step % PRINT_EVERY == 0:\n",
    "                print (\"Episode: {} ... Avg reward: {:.2f}\".format(opt_step,np.mean(scores_window)))\n",
    "                # save the policy\n",
    "                torch.save(agent.policy, 'REINFORCE-Ant.policy')\n",
    "            \n",
    "            if np.mean(scores_window)>= 500.0:\n",
    "                print (\"Environment solved in {} optimization steps! ... Avg reward : {:.2f}\".format(opt_step-100,\n",
    "                                                                                          np.mean(scores_window)))\n",
    "                # save the policy\n",
    "                torch.save(agent.policy, 'REINFORCE-Ant.policy')\n",
    "                break\n",
    "                \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets define and train our agent\n",
    "agent = Agent(env_name='Ant-v2',learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = agent.train(max_opt_steps=20000,horizon=800,gamma=0.98,PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reward curve over episodes\n",
    "plt.figure()\n",
    "plt.plot(scores)\n",
    "plt.xlabel('Episode #')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Watch the smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell to load the trained policy for Pendulum-v0\n",
    "# load policy\n",
    "policy =  torch.load('REINFORCE-Ant.policy',map_location='cpu')\n",
    "agent = Agent(env_name='Ant-v2')\n",
    "agent.policy = policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.figure(dpi = 72)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "total_reward = 0\n",
    "state = env.reset()\n",
    "for t in range(2000):\n",
    "    action, _ = agent.policy.act(state[np.newaxis,:])\n",
    "    frames.append(env.render(mode='rgb_array')) \n",
    "    next_state, reward, done, _ = env.step(action[0])\n",
    "    state=next_state\n",
    "    total_reward+= reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print (\"Total reward:\",total_reward)\n",
    "env.close()\n",
    "animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
