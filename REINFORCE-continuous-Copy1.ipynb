{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "# setting manual seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from parallelEnv import parallelEnv\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# imports for rendering outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(1,) ... State space: Box(3,)\n",
      "action_high: [2.]\t action_low: [-2.]\n"
     ]
    }
   ],
   "source": [
    "# lets set up the cartpole environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# lets find about the action and state space\n",
    "print(\"Action space: {} ... State space: {}\".format(env.action_space,env.observation_space))\n",
    "\n",
    "# lets print some info about action space range\n",
    "action_high = env.action_space.high\n",
    "action_low = env.action_space.low\n",
    "print(\"action_high: {}\\t action_low: {}\".format(action_high,action_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0 ... state: [-0.94223519 -0.33495202  0.93078187] ... action: [0.19525401] ... reward: -7.926888357788497 ... done: False\n",
      "Step : 1 ... state: [-0.92977428 -0.36813012  0.70885596] ... action: [0.86075747] ... reward: -7.6939771290583225 ... done: False\n",
      "Step : 2 ... state: [-0.91906665 -0.39410213  0.56187199] ... action: [0.4110535] ... reward: -7.52018189583656 ... done: False\n",
      "Step : 3 ... state: [-0.91248103 -0.40911902  0.32795343] ... action: [0.17953274] ... reward: -7.409754858726039 ... done: False\n",
      "Step : 4 ... state: [-0.91149561 -0.4113098   0.04804407] ... action: [-0.3053808] ... reward: -7.386228644680105 ... done: False\n",
      "Step : 5 ... state: [-0.91768659 -0.39730506 -0.3062454 ] ... action: [0.58357644] ... reward: -7.47908659136635 ... done: False\n",
      "Step : 6 ... state: [-0.92764336 -0.37346725 -0.51668773] ... action: [-0.24965115] ... reward: -7.63800526593766 ... done: False\n",
      "Step : 7 ... state: [-0.94240996 -0.33445996 -0.83423584] ... action: [1.5670921] ... reward: -7.915189964562505 ... done: False\n",
      "Step : 8 ... state: [-0.95576949 -0.29411677 -0.85001701] ... action: [1.8546511] ... reward: -8.158690593416598 ... done: False\n",
      "Step : 9 ... state: [-0.96666938 -0.25602794 -0.79240692] ... action: [-0.46623394] ... reward: -8.372862139443372 ... done: False\n",
      "Step : 10 ... state: [-0.97881748 -0.20473483 -1.05436296] ... action: [1.1669002] ... reward: -8.729102160787 ... done: False\n",
      "Step : 11 ... state: [-0.98808108 -0.15393433 -1.03287906] ... action: [0.11557968] ... reward: -9.02912805491034 ... done: False\n",
      "Step : 12 ... state: [-0.99520192 -0.09784241 -1.13099285] ... action: [0.27217823] ... reward: -9.391449727150244 ... done: False\n",
      "Step : 13 ... state: [-0.99920722 -0.03981127 -1.16354792] ... action: [1.7023865] ... reward: -9.759264952168081 ... done: False\n",
      "Step : 14 ... state: [-0.99997493  0.00708057 -0.9380484 ] ... action: [-1.7158557] ... reward: -9.916103301248963 ... done: False\n",
      "Step : 15 ... state: [-0.99778394  0.06653725 -1.19011633] ... action: [-1.6514828] ... reward: -9.600028291306998 ... done: False\n",
      "Step : 16 ... state: [-0.99076852  0.13556453 -1.38793582] ... action: [-1.9191264] ... reward: -9.230007625717896 ... done: False\n",
      "Step : 17 ... state: [-0.97704253  0.21304436 -1.57413138] ... action: [1.3304794] ... reward: -8.81631944495825 ... done: False\n",
      "Step : 18 ... state: [-0.96230872  0.27195942 -1.2147762 ] ... action: [1.112627] ... reward: -8.363702381066593 ... done: False\n",
      "Step : 19 ... state: [-0.94998007  0.31231052 -0.84391258] ... action: [1.4800485] ... reward: -8.048206990140496 ... done: False\n",
      "Step : 20 ... state: [-0.94374828  0.33066475 -0.38767241] ... action: [1.9144734] ... reward: -7.884390060257875 ... done: False\n",
      "Step : 21 ... state: [-0.9461612   0.32369581  0.14749716] ... action: [1.1966343] ... reward: -7.910724555242787 ... done: False\n",
      "Step : 22 ... state: [-0.95499755  0.29661368  0.56976416] ... action: [-0.15408255] ... reward: -8.10063090433239 ... done: False\n",
      "Step : 23 ... state: [-0.96569515  0.25967843  0.76911204] ... action: [1.1221167] ... reward: -8.348497619215824 ... done: False\n",
      "Step : 24 ... state: [-0.97884061  0.20462421  1.13218836] ... action: [-1.5269023] ... reward: -8.747752345871733 ... done: False\n",
      "Step : 25 ... state: [-0.98828038  0.15264957  1.05662117] ... action: [0.5596841] ... reward: -9.042157567976504 ... done: False\n",
      "Step : 26 ... state: [-0.99590806  0.0903722   1.25506096] ... action: [-1.4265869] ... reward: -9.468745772296735 ... done: False\n",
      "Step : 27 ... state: [-0.9993857   0.03504589  1.10885208] ... action: [1.7786757] ... reward: -9.776707208887354 ... done: False\n",
      "Step : 28 ... state: [-0.99938603 -0.03503666  1.40193785] ... action: [0.08739328] ... reward: -9.847196208193676 ... done: False\n",
      "Step : 29 ... state: [-0.99454669 -0.1042923   1.38876935] ... action: [-0.34135225] ... reward: -9.417023871066851 ... done: False\n",
      "Step : 30 ... state: [-0.98601303 -0.16666823  1.25934728] ... action: [-0.9417775] ... reward: -9.005009043383941 ... done: False\n",
      "Step : 31 ... state: [-0.97652543 -0.21540216  0.99307948] ... action: [1.0969348] ... reward: -8.652459970694382 ... done: False\n",
      "Step : 32 ... state: [-0.96459127 -0.26374926  0.99606809] ... action: [-0.17539868] ... reward: -8.363063307425167 ... done: False\n",
      "Step : 33 ... state: [-0.95369538 -0.30077422  0.77194634] ... action: [0.2737358] ... reward: -8.103061874939415 ... done: False\n",
      "Step : 34 ... state: [-0.94445118 -0.32865174  0.58742605] ... action: [-1.9248408] ... reward: -7.915872236472327 ... done: False\n",
      "Step : 35 ... state: [-0.94359    -0.33111616  0.05221112] ... action: [0.47054198] ... reward: -7.863506928985015 ... done: False\n",
      "Step : 36 ... state: [-0.94564989 -0.32518654 -0.1255447 ] ... action: [0.44838288] ... reward: -7.900033587216993 ... done: False\n",
      "Step : 37 ... state: [-0.95045497 -0.31086228 -0.30217717] ... action: [0.46773598] ... reward: -7.992757930171936 ... done: False\n",
      "Step : 38 ... state: [-0.95742735 -0.28867435 -0.46516348] ... action: [1.7749923] ... reward: -8.140168889734982 ... done: False\n",
      "Step : 39 ... state: [-0.96321645 -0.26872676 -0.41542039] ... action: [0.7272812] ... reward: -8.251941280714608 ... done: False\n",
      "Step : 40 ... state: [-0.96972913 -0.24418316 -0.50787328] ... action: [-0.5619684] ... reward: -8.406644446680357 ... done: False\n",
      "Step : 41 ... state: [-0.97846405 -0.20641729 -0.77530591] ... action: [-0.25187218] ... reward: -8.66665572368436 ... done: False\n",
      "Step : 42 ... state: [-0.98730412 -0.15884134 -0.9678997 ] ... action: [0.7905248] ... reward: -8.98708321112506 ... done: False\n",
      "Step : 43 ... state: [-0.99383536 -0.11086601 -0.96845199] ... action: [-1.7590982] ... reward: -9.28080410974373 ... done: False\n",
      "Step : 44 ... state: [-0.99897318 -0.04530556 -1.31546622] ... action: [0.6670669] ... reward: -9.760387811127535 ... done: False\n",
      "Step : 45 ... state: [-0.99985297  0.01714735 -1.24938536] ... action: [0.6825515] ... reward: -9.918715434142808 ... done: False\n",
      "Step : 46 ... state: [-0.99727394  0.07378818 -1.13414212] ... action: [-1.1584698] ... reward: -9.540982357550146 ... done: False\n",
      "Step : 47 ... state: [-0.99070053  0.13606053 -1.25257145] ... action: [-1.4842948] ... reward: -9.189775441404906 ... done: False\n",
      "Step : 48 ... state: [-0.97903199  0.20370656 -1.37317027] ... action: [-0.7382866] ... reward: -8.811844899882612 ... done: False\n",
      "Step : 49 ... state: [-0.96331631  0.26836855 -1.33113334] ... action: [-0.5451569] ... reward: -8.413778071598886 ... done: False\n",
      "Step : 50 ... state: [-0.94530088  0.32619971 -1.21163046] ... action: [0.28078708] ... reward: -8.039118027875363 ... done: False\n",
      "Step : 51 ... state: [-0.92921121  0.36954908 -0.92486262] ... action: [-0.24559395] ... reward: -7.720147405978615 ... done: False\n",
      "Step : 52 ... state: [-0.9160209   0.40113054 -0.6845399 ] ... action: [1.9534954] ... reward: -7.4972540276175765 ... done: False\n",
      "Step : 53 ... state: [-0.91419301  0.40527908 -0.09066769] ... action: [-1.5918207] ... reward: -7.425213125263578 ... done: False\n",
      "Step : 54 ... state: [-0.91367592  0.4064435  -0.02548148] ... action: [-1.164493] ... reward: -7.416337824987945 ... done: False\n",
      "Step : 55 ... state: [-0.91579066  0.40165591  0.1046772 ] ... action: [-1.354762] ... reward: -7.446379318054962 ... done: False\n",
      "Step : 56 ... state: [-0.91981443  0.39235367  0.20270484] ... action: [0.6124333] ... reward: -7.503338247861507 ... done: False\n",
      "Step : 57 ... state: [-0.93096572  0.3651066   0.58883509] ... action: [-0.9868336] ... reward: -7.696614394085901 ... done: False\n",
      "Step : 58 ... state: [-0.94341468  0.33161536  0.71464   ] ... action: [-0.13475691] ... reward: -7.911135146492003 ... done: False\n",
      "Step : 59 ... state: [-0.95799806  0.28677468  0.94313798] ... action: [-1.0222976] ... reward: -8.216681773175173 ... done: False\n",
      "Step : 60 ... state: [-0.97119168  0.23829965  1.00487435] ... action: [-1.3641217] ... reward: -8.518511232879423 ... done: False\n",
      "Step : 61 ... state: [-0.9816883   0.1904943   0.97898084] ... action: [-1.5584995] ... reward: -8.800338619810168 ... done: False\n",
      "Step : 62 ... state: [-0.98917656  0.14673014  0.88807665] ... action: [0.62531835] ... reward: -9.045276215984787 ... done: False\n",
      "Step : 63 ... state: [-0.9957096   0.09253315  1.091922  ] ... action: [-1.4472682] ... reward: -9.417279437070684 ... done: False\n",
      "Step : 64 ... state: [-0.99896714  0.04543848  0.94423163] ... action: [-1.2136706] ... reward: -9.676704076885416 ... done: False\n",
      "Step : 65 ... state: [-0.99998409  0.00564111  0.7962599 ] ... action: [-0.52509934] ... reward: -9.897870623971073 ... done: False\n",
      "Step : 66 ... state: [-0.99953658 -0.03044045  0.72172583] ... action: [1.283973] ... reward: -9.732976167665518 ... done: False\n",
      "Step : 67 ... state: [-0.99718734 -0.07494938  0.89149143] ... action: [-1.6115949] ... reward: -9.485942457831673 ... done: False\n",
      "Step : 68 ... state: [-0.9945243  -0.10450557  0.59354016] ... action: [1.3517796] ... reward: -9.259793127358815 ... done: False\n",
      "Step : 69 ... state: [-0.99013305 -0.14013042  0.71792793] ... action: [-1.6156064] ... reward: -9.060150373939829 ... done: False\n",
      "Step : 70 ... state: [-0.98736748 -0.158447    0.37048915] ... action: [1.9058379] ... reward: -8.912515957954072 ... done: False\n",
      "Step : 71 ... state: [-0.98275291 -0.18492355  0.53752958] ... action: [-0.1253952] ... reward: -8.764472536021078 ... done: False\n",
      "Step : 72 ... state: [-0.97906191 -0.2035627   0.38002764] ... action: [1.9070444] ... reward: -8.641682062941298 ... done: False\n",
      "Step : 73 ... state: [-0.97351433 -0.228626    0.51341227] ... action: [0.41938207] ... reward: -8.50002871678587 ... done: False\n",
      "Step : 74 ... state: [-0.96868724 -0.24828418  0.40485009] ... action: [0.9570543] ... reward: -8.373360801777412 ... done: False\n",
      "Step : 75 ... state: [-0.96403228 -0.2657852   0.3621951 ] ... action: [-1.8432488] ... reward: -8.268199386530064 ... done: False\n",
      "Step : 76 ... state: [-0.96552678 -0.26030373 -0.11363112] ... action: [-0.86877215] ... reward: -8.28640292378813 ... done: False\n",
      "Step : 77 ... state: [-0.97100949 -0.23904093 -0.43917474] ... action: [-1.5192138] ... reward: -8.432840326742499 ... done: False\n",
      "Step : 78 ... state: [-0.98025267 -0.19774911 -0.84633751] ... action: [-0.8154392] ... reward: -8.730785290877906 ... done: False\n",
      "Step : 79 ... state: [-0.98976255 -0.14272385 -1.11696523] ... action: [-1.5250891] ... reward: -9.117368225599487 ... done: False\n",
      "Step : 80 ... state: [-0.99751067 -0.07051575 -1.45277148] ... action: [-0.7280673] ... reward: -9.64273820119626 ... done: False\n",
      "Step : 81 ... state: [-0.9999483   0.01016893 -1.61486839] ... action: [-0.34294802] ... reward: -10.06671106431905 ... done: False\n",
      "Step : 82 ... state: [-0.99566902  0.09296885 -1.65868389] ... action: [-1.74341] ... reward: -9.571450064580587 ... done: False\n",
      "Step : 83 ... state: [-0.98282077  0.18456254 -1.85046876] ... action: [0.76988846] ... reward: -9.080750705862837 ... done: False\n",
      "Step : 84 ... state: [-0.96497325  0.26234827 -1.59656358] ... action: [0.26640582] ... reward: -8.52714079953506 ... done: False\n",
      "Step : 85 ... state: [-0.94491975  0.32730209 -1.35984151] ... action: [-0.93844205] ... reward: -8.071479639876328 ... done: False\n",
      "Step : 86 ... state: [-0.92253276  0.38591878 -1.25513124] ... action: [0.09299222] ... reward: -7.694705415919413 ... done: False\n",
      "Step : 87 ... state: [-0.90313055  0.42936605 -0.95174332] ... action: [-1.624238] ... reward: -7.371354972888987 ... done: False\n",
      "Step : 88 ... state: [-0.88352613  0.46838187 -0.87335448] ... action: [0.30378598] ... reward: -7.120795508275507 ... done: False\n",
      "Step : 89 ... state: [-0.87211724  0.48929697 -0.47650018] ... action: [1.7171848] ... reward: -6.944180469882488 ... done: False\n",
      "Step : 90 ... state: [-0.87571534  0.48282776  0.14805026] ... action: [-0.72572416] ... reward: -6.960241660338585 ... done: False\n",
      "Step : 91 ... state: [-0.88522664  0.46515997  0.40131246] ... action: [0.6696415] ... reward: -7.08033397165668 ... done: False\n",
      "Step : 92 ... state: [-0.90420406  0.42710071  0.85062866] ... action: [-1.4728086] ... reward: -7.366193540316925 ... done: False\n",
      "Step : 93 ... state: [-0.92346449  0.38368391  0.9500329 ] ... action: [0.8653088] ... reward: -7.6414670978830275 ... done: False\n",
      "Step : 94 ... state: [-0.94752209  0.31969031  1.36759216] ... action: [-0.84237564] ... reward: -8.118666683727888 ... done: False\n",
      "Step : 95 ... state: [-0.96857693  0.24871414  1.48100355] ... action: [-1.2672346] ... reward: -8.574431459920598 ... done: False\n",
      "Step : 96 ... state: [-0.98429177  0.17654948  1.47745397] ... action: [0.34605175] ... reward: -9.004372060799104 ... done: False\n",
      "Step : 97 ... state: [-0.99554847  0.09425096  1.66177384] ... action: [-1.9195698] ... reward: -9.565271521907535 ... done: False\n",
      "Step : 98 ... state: [-0.99975437  0.02216292  1.44452658] ... action: [1.3157601] ... reward: -9.941227484976244 ... done: False\n",
      "Step : 99 ... state: [-0.99815462 -0.06072353  1.65851279] ... action: [-1.9812181] ... reward: -9.770515941837829 ... done: False\n",
      "Step : 100 ... state: [-0.99200319 -0.12621277  1.31578743] ... action: [0.71126616] ... reward: -9.264116078519274 ... done: False\n",
      "Step : 101 ... state: [-0.98144452 -0.19174631  1.32781778] ... action: [-0.9199681] ... reward: -8.871701933470291 ... done: False\n",
      "Step : 102 ... state: [-0.97007864 -0.24279091  1.04601282] ... action: [0.9407761] ... reward: -8.49914755992304 ... done: False\n",
      "Step : 103 ... state: [-0.95665851 -0.2912121   1.00503606] ... action: [1.8487542] ... reward: -8.204707397150514 ... done: False\n",
      "Step : 104 ... state: [-0.93982089 -0.34166752  1.06394011] ... action: [-1.0049875] ... reward: -7.914508641073295 ... done: False\n",
      "Step : 105 ... state: [-0.92809318 -0.37234802  0.65694135] ... action: [0.30462933] ... reward: -7.661153424846919 ... done: False\n",
      "Step : 106 ... state: [-0.92000369 -0.39190969  0.42337473] ... action: [0.36816773] ... reward: -7.519557958046227 ... done: False\n",
      "Step : 107 ... state: [-0.91634587 -0.40038761  0.18466762] ... action: [0.28900763] ... reward: -7.45449842730149 ... done: False\n",
      "Step : 108 ... state: [-0.91778673 -0.3970737  -0.07227194] ... action: [-1.1076735] ... reward: -7.472494741730429 ... done: False\n",
      "Step : 109 ... state: [-0.9281017  -0.37232678 -0.53622824] ... action: [1.810996] ... reward: -7.650063575696138 ... done: False\n",
      "Step : 110 ... state: [-0.93788139 -0.34695606 -0.54382392] ... action: [-0.21149848] ... reward: -7.798487969955936 ... done: False\n",
      "Step : 111 ... state: [-0.95155709 -0.30747211 -0.83576574] ... action: [1.3856347] ... reward: -8.07533587086507 ... done: False\n",
      "Step : 112 ... state: [-0.96387509 -0.26635466 -0.85852462] ... action: [0.7979171] ... reward: -8.322632775388662 ... done: False\n",
      "Step : 113 ... state: [-0.97530933 -0.22084319 -0.93860305] ... action: [-0.8102522] ... reward: -8.60881141997184 ... done: False\n",
      "Step : 114 ... state: [-0.98700484 -0.16069056 -1.22577327] ... action: [1.2551913] ... reward: -9.033433802945796 ... done: False\n",
      "Step : 115 ... state: [-0.99464973 -0.10330501 -1.15801249] ... action: [-0.41397703] ... reward: -9.364340597984363 ... done: False\n",
      "Step : 116 ... state: [-0.99925472 -0.03860066 -1.2975878 ] ... action: [1.5244128] ... reward: -9.799197048247581 ... done: False\n",
      "Step : 117 ... state: [-0.99986743  0.01628285 -1.09787638] ... action: [0.32509148] ... reward: -9.8881958061179 ... done: False\n",
      "Step : 118 ... state: [-0.99768014  0.06807591 -1.03690052] ... action: [1.5269414] ... reward: -9.55602913139217 ... done: False\n",
      "Step : 119 ... state: [-0.99439057  0.1057705  -0.75680238] ... action: [0.77012634] ... reward: -9.272880774858042 ... done: False\n",
      "Step : 120 ... state: [-0.99102654  0.13366524 -0.56195555] ... action: [0.9010171] ... reward: -9.077604844896856 ... done: False\n",
      "Step : 121 ... state: [-0.98871209  0.14982789 -0.32655405] ... action: [0.00529753] ... reward: -8.9579319997716 ... done: False\n",
      "Step : 122 ... state: [-0.98705727  0.16036815 -0.21338851] ... action: [1.8243345] ... reward: -8.89143497786355 ... done: False\n",
      "Step : 123 ... state: [-0.98846466  0.15145168  0.18053778] ... action: [0.5759608] ... reward: -8.94103573070188 ... done: False\n",
      "Step : 124 ... state: [-0.99116711  0.13261884  0.38052067] ... action: [-0.3045798] ... reward: -9.066137748330444 ... done: False\n",
      "Step : 125 ... state: [-0.99381301  0.11106618  0.43429783] ... action: [0.42557284] ... reward: -9.201741552495157 ... done: False\n",
      "Step : 126 ... state: [-0.9966215   0.08213152  0.58143339] ... action: [-1.9232272] ... reward: -9.39724100171635 ... done: False\n",
      "Step : 127 ... state: [-0.99792081  0.06445203  0.35454795] ... action: [-0.79370075] ... reward: -9.481719651172417 ... done: False\n",
      "Step : 128 ... state: [-0.99873496  0.05028393  0.28383187] ... action: [0.64069414] ... reward: -9.564524995493366 ... done: False\n",
      "Step : 129 ... state: [-0.99956718  0.02941845  0.41764894] ... action: [-0.83968955] ... reward: -9.703749965785132 ... done: False\n",
      "Step : 130 ... state: [-0.99990568  0.0137343   0.31375934] ... action: [0.47206172] ... reward: -9.793562511105048 ... done: False\n",
      "Step : 131 ... state: [-0.99998195 -0.0060087   0.39486933] ... action: [-0.2849252] ... reward: -9.847559876875428 ... done: False\n",
      "Step : 132 ... state: [-0.99972647 -0.0233878   0.34762402] ... action: [-1.4581038] ... reward: -9.737398504850411 ... done: False\n",
      "Step : 133 ... state: [-0.99958074 -0.02895427  0.11136761] ... action: [-0.8068707] ... reward: -9.690383836311229 ... done: False\n",
      "Step : 134 ... state: [-0.99962493 -0.02738596 -0.0313787 ] ... action: [0.27985963] ... reward: -9.698438808796396 ... done: False\n",
      "Step : 135 ... state: [-0.99963842 -0.02688918 -0.00993922] ... action: [0.36349106] ... reward: -9.701499552543106 ... done: False\n",
      "Step : 136 ... state: [-0.99960485 -0.02810959  0.02441755] ... action: [0.297301] ... reward: -9.6939017123977 ... done: False\n",
      "Step : 137 ... state: [-0.99953461 -0.03050509  0.04793051] ... action: [0.6128033] ... reward: -9.67944164318886 ... done: False\n",
      "Step : 138 ... state: [-0.99933911 -0.03635042  0.11697218] ... action: [0.6084131] ... reward: -9.6442179893425 ... done: False\n",
      "Step : 139 ... state: [-0.99896928 -0.0453914   0.18097133] ... action: [-0.27432626] ... reward: -9.589715930639462 ... done: False\n",
      "Step : 140 ... state: [-0.99871524 -0.05067423  0.10577884] ... action: [1.5861864] ... reward: -9.55727739640421 ... done: False\n",
      "Step : 141 ... state: [-0.99782404 -0.06593313  0.30570113] ... action: [-0.5297525] ... reward: -9.469013018704995 ... done: False\n",
      "Step : 142 ... state: [-0.99720226 -0.07475063  0.17678841] ... action: [-0.2565403] ... reward: -9.408283175582488 ... done: False\n",
      "Step : 143 ... state: [-0.99688644 -0.0788507   0.08224439] ... action: [1.5676935] ... reward: -9.383020453141054 ... done: False\n",
      "Step : 144 ... state: [-0.99578515 -0.09171658  0.25826039] ... action: [1.2247759] ... reward: -9.309126664129945 ... done: False\n",
      "Step : 145 ... state: [-0.99390052 -0.11028036  0.37318934] ... action: [0.8155543] ... reward: -9.202083771034237 ... done: False\n",
      "Step : 146 ... state: [-0.99141272 -0.13077012  0.41281222] ... action: [-1.5990925] ... reward: -9.082389140931769 ... done: False\n",
      "Step : 147 ... state: [-0.99091623 -0.13448059  0.07487075] ... action: [1.6779305] ... reward: -9.043641235622706 ... done: False\n",
      "Step : 148 ... state: [-0.98933555 -0.14565427  0.22569988] ... action: [0.8569652] ... reward: -8.978359745847825 ... done: False\n",
      "Step : 149 ... state: [-0.98747707 -0.1577626   0.24500396] ... action: [1.995388] ... reward: -8.909276711937888 ... done: False\n",
      "Step : 150 ... state: [-0.98389307 -0.178758    0.42599021] ... action: [-1.4022068] ... reward: -8.792779018819585 ... done: False\n",
      "Step : 151 ... state: [-0.98315564 -0.18277033  0.08159069] ... action: [1.4725043] ... reward: -8.751350592190033 ... done: False\n",
      "Step : 152 ... state: [-0.98161063 -0.19089412  0.16538859] ... action: [-1.3500283] ... reward: -8.704224460883973 ... done: False\n",
      "Step : 153 ... state: [-0.98329151 -0.18203794 -0.18028625] ... action: [0.46223825] ... reward: -8.756387565082608 ... done: False\n",
      "Step : 154 ... state: [-0.9854687  -0.16985712 -0.24747897] ... action: [-1.5047201] ... reward: -8.834682962792892 ... done: False\n",
      "Step : 155 ... state: [-0.99012429 -0.14019236 -0.60057982] ... action: [1.3920329] ... reward: -9.043630040949772 ... done: False\n",
      "Step : 156 ... state: [-0.99330154 -0.11555103 -0.49691916] ... action: [1.2292758] ... reward: -9.181566285656572 ... done: False\n",
      "Step : 157 ... state: [-0.99540989 -0.09570348 -0.39919106] ... action: [0.27640295] ... reward: -9.292558982469492 ... done: False\n",
      "Step : 158 ... state: [-0.99723547 -0.07430622 -0.42950822] ... action: [-0.3712668] ... reward: -9.426411141298424 ... done: False\n",
      "Step : 159 ... state: [-0.99888022 -0.0473107  -0.54092791] ... action: [-1.723332] ... reward: -9.606701626854303 ... done: False\n",
      "Step : 160 ... state: [-0.99998442 -0.0055828  -0.83491074] ... action: [0.7897151] ... reward: -9.90488883650544 ... done: False\n",
      "Step : 161 ... state: [-0.99953646  0.03044449 -0.72064058] ... action: [-0.18582927] ... reward: -9.731180433974506 ... done: False\n",
      "Step : 162 ... state: [-0.99777416  0.06668376 -0.7256816 ] ... action: [0.8882224] ... reward: -9.508210512900764 ... done: False\n",
      "Step : 163 ... state: [-0.99559885  0.09371732 -0.54243542] ... action: [1.4655293] ... reward: -9.320275915973424 ... done: False\n",
      "Step : 164 ... state: [-0.99433732  0.1062699  -0.25231804] ... action: [1.902086] ... reward: -9.221948134860389 ... done: False\n",
      "Step : 165 ... state: [-0.99492035  0.10066529  0.11269729] ... action: [1.4232134] ... reward: -9.249496119768041 ... done: False\n",
      "Step : 166 ... state: [-0.99674131  0.08066444  0.40167826] ... action: [-1.9531437] ... reward: -9.388693773567462 ... done: False\n",
      "Step : 167 ... state: [-0.99738808  0.07222897  0.16920503] ... action: [-0.56008774] ... reward: -9.423783730220357 ... done: False\n",
      "Step : 168 ... state: [-0.99786716  0.06527729  0.1393636 ] ... action: [0.9199622] ... reward: -9.466218983993492 ... done: False\n",
      "Step : 169 ... state: [-0.99879935  0.04898833  0.3263159 ] ... action: [-1.3134813] ... reward: -9.576453623090371 ... done: False\n",
      "Step : 170 ... state: [-0.99917162  0.04069496  0.16603495] ... action: [0.08414643] ... reward: -9.618260656995925 ... done: False\n",
      "Step : 171 ... state: [-0.99954259  0.03024268  0.20917813] ... action: [-1.7826481] ... reward: -9.688023350218586 ... done: False\n",
      "Step : 172 ... state: [-0.99948727  0.03201867 -0.03553708] ... action: [-1.2000139] ... reward: -9.670982634295747 ... done: False\n",
      "Step : 173 ... state: [-0.99913483  0.0415884  -0.19152515] ... action: [-1.9259129] ... reward: -9.617329287314636 ... done: False\n",
      "Step : 174 ... state: [-0.99794877  0.06401763 -0.44922078] ... action: [1.1747909] ... reward: -9.492758427668965 ... done: False\n",
      "Step : 175 ... state: [-0.99716548  0.07523972 -0.22498893] ... action: [-1.1043012] ... reward: -9.408365352067152 ... done: False\n",
      "Step : 176 ... state: [-0.99576905  0.09189129 -0.33420432] ... action: [-0.6185933] ... reward: -9.311438549771168 ... done: False\n",
      "Step : 177 ... state: [-0.99396435  0.1097036  -0.35807485] ... action: [1.7123252] ... reward: -9.206763514439467 ... done: False\n",
      "Step : 178 ... state: [-0.99385996  0.11064525 -0.01894836] ... action: [0.8176576] ... reward: -9.18597054883214 ... done: False\n",
      "Step : 179 ... state: [-0.99484944  0.10136367  0.18668421] ... action: [-1.8726443] ... reward: -9.248923870046237 ... done: False\n",
      "Step : 180 ... state: [-0.99475684  0.10226842 -0.01818968] ... action: [-1.3412234] ... reward: -9.238235005077913 ... done: False\n",
      "Step : 181 ... state: [-0.99400199  0.10936195 -0.14267187] ... action: [0.4859136] ... reward: -9.195365514256727 ... done: False\n",
      "Step : 182 ... state: [-0.99406872  0.10875377  0.01223664] ... action: [0.30891436] ... reward: -9.196914815163666 ... done: False\n",
      "Step : 183 ... state: [-0.99480634  0.10178576  0.14013912] ... action: [-1.0484288] ... reward: -9.24241552859704 ... done: False\n",
      "Step : 184 ... state: [-0.99510334  0.09883999  0.05921413] ... action: [1.736856] ... reward: -9.260727399754595 ... done: False\n",
      "Step : 185 ... state: [-0.99685677  0.0792249   0.39387252] ... action: [0.45586383] ... reward: -9.39330860505856 ... done: False\n",
      "Step : 186 ... state: [-0.99858391  0.05319935  0.52167077] ... action: [0.14253122] ... reward: -9.565252379958329 ... done: False\n",
      "Step : 187 ... state: [-0.99971016  0.02407465  0.58294996] ... action: [0.3596399] ... reward: -9.753016419220149 ... done: False\n",
      "Step : 188 ... state: [-0.99996241 -0.00867051  0.65495193] ... action: [0.9204881] ... reward: -9.85894395363763 ... done: False\n",
      "Step : 189 ... state: [-0.99884838 -0.04797831  0.78652226] ... action: [-0.75222003] ... reward: -9.632763264532562 ... done: False\n",
      "Step : 190 ... state: [-0.99681113 -0.07979708  0.63770553] ... action: [-0.40711576] ... reward: -9.414904629054677 ... done: False\n",
      "Step : 191 ... state: [-0.99441668 -0.1055247   0.51679035] ... action: [-1.160625] ... reward: -9.244567758103598 ... done: False\n",
      "Step : 192 ... state: [-0.99293982 -0.11861924  0.26355308] ... action: [-1.2552279] ... reward: -9.145197432839888 ... done: False\n",
      "Step : 193 ... state: [-0.99302081 -0.11793927 -0.01369553] ... action: [1.7774895] ... reward: -9.143994248117247 ... done: False\n",
      "Step : 194 ... state: [-0.99201735 -0.12610146  0.16447345] ... action: [0.9582032] ... reward: -9.094780411439153 ... done: False\n",
      "Step : 195 ... state: [-0.99061385 -0.13669019  0.21362783] ... action: [-0.03816476] ... reward: -9.031424247245464 ... done: False\n",
      "Step : 196 ... state: [-0.98987984 -0.14190809  0.10538547] ... action: [-1.0903414] ... reward: -8.997523543195816 ... done: False\n",
      "Step : 197 ... state: [-0.99101418 -0.13375682 -0.16459681] ... action: [-0.98257405] ... reward: -9.04833242236538 ... done: False\n",
      "Step : 198 ... state: [-0.99356082 -0.11330006 -0.41230053] ... action: [-1.7678833] ... reward: -9.189203977467072 ... done: False\n",
      "Step : 199 ... state: [-0.99715719 -0.07534949 -0.76245808] ... action: [-0.2623335] ... reward: -9.459611823943584 ... done: True\n"
     ]
    }
   ],
   "source": [
    "# lets play a random episode\n",
    "\n",
    "# reset env\n",
    "state = env.reset()\n",
    "step_count = 0\n",
    "done = False\n",
    "\n",
    "while (not done):\n",
    "    # random action\n",
    "    action = env.action_space.sample()\n",
    "    # taking step in env\n",
    "    next_state,reward,done,_ = env.step(action)\n",
    "    \n",
    "    # displaying progress\n",
    "    print (\"Step : {} ... state: {} ... action: {} ... reward: {} ... done: {}\".format(step_count,\n",
    "                                                                                       state,action,reward,done))\n",
    "    \n",
    "    # updating state and step count\n",
    "    state = next_state\n",
    "    step_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"using\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# define actor critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_size,action_size,action_high,action_low,hidden_size=32):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # action range\n",
    "        self.action_high = torch.tensor(action_high).to(device)\n",
    "        self.action_low = torch.tensor(action_low).to(device)\n",
    "        \n",
    "        # common network\n",
    "        self.fc1 = nn.Linear(state_size,128)\n",
    "        \n",
    "        # actor network\n",
    "        self.fc2_actor = nn.Linear(128,64)\n",
    "        self.fc3_action = nn.Linear(64,action_size)\n",
    "        self.fc3_std = nn.Linear(64,action_size)\n",
    "        \n",
    "        # critic network\n",
    "        self.fc2_critic = nn.Linear(128,64)\n",
    "        self.fc3_critic = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        # common network\n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        # actor network\n",
    "        x_actor = F.relu(self.fc2_actor(x))\n",
    "        action_mean = F.sigmoid(self.fc3_action(x_actor))\n",
    "        ## rescale action mean\n",
    "        action_mean_ = (self.action_high-self.action_low)*action_mean + self.action_low\n",
    "        action_std = F.sigmoid(self.fc3_std(x_actor))\n",
    "        \n",
    "        # critic network\n",
    "        x_critic = F.relu(self.fc2_critic(x))\n",
    "        v = self.fc3_critic(x_critic)\n",
    "        return action_mean_,action_std,v\n",
    "    \n",
    "    def act(self,state):\n",
    "        # converting state from numpy array to pytorch tensor on the \"device\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_mean,action_std,v = self.forward(state)\n",
    "        prob_dist = Normal(action_mean,action_std)\n",
    "        action = prob_dist.sample()\n",
    "        log_prob = prob_dist.log_prob(action)\n",
    "        return action.cpu().numpy(),log_prob,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import accumulate\n",
    "\n",
    "def compute_future_rewards(rewards,gamma):\n",
    "    future_rewards = np.zeros_like(rewards)\n",
    "    discounted_rewards = np.zeros(rewards.shape[0])\n",
    "    \n",
    "    for time_step in range(future_rewards.shape[1]-1,-1,-1):\n",
    "        future_rewards[:,time_step] = rewards[:,time_step] + gamma*discounted_rewards\n",
    "        discounted_rewards = future_rewards[:,time_step]\n",
    "    return future_rewards\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self,env_name,learning_rate=1e-3):\n",
    "        self.env = parallelEnv(env_name=env_name,n=8,seed=0)\n",
    "        nS = env.observation_space.shape[0]\n",
    "        nA = env.action_space.shape[0]\n",
    "        self.policy = ActorCritic(state_size=nS,hidden_size=128,action_size=nA,\n",
    "                             action_low=action_low,action_high=action_high).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def train(self,max_opt_steps=1000,num_trajectories=8,horizon=1000,gamma=.99,PRINT_EVERY=100):\n",
    "        # store eps scores\n",
    "        scores = []\n",
    "        scores_window = deque(maxlen=100)\n",
    "        \n",
    "        for opt_step in range(1,max_opt_steps+1):\n",
    "            rewards = np.zeros([num_trajectories,horizon])\n",
    "            log_probs = torch.zeros([num_trajectories,horizon],dtype=torch.double,device=device)\n",
    "            value_estimate = torch.zeros([num_trajectories,horizon],dtype=torch.double,device=device)\n",
    "            \n",
    "            for traj_count in range(1):\n",
    "                # reset state\n",
    "                state = self.env.reset()\n",
    "            \n",
    "                # play an episode\n",
    "                for t in range(horizon): \n",
    "                    action,log_prob,v = self.policy.act(state)\n",
    "                    next_state,reward,done,_ = self.env.step(action)\n",
    "\n",
    "                    # update state\n",
    "                    state = next_state\n",
    "                    log_probs[:,t] = log_prob.squeeze()\n",
    "                    rewards[:,t] = reward\n",
    "                    value_estimate[:,t] = v.squeeze()\n",
    "                    \n",
    "                    # break if done\n",
    "                    if np.any(done):\n",
    "                        break\n",
    "            \n",
    "            # compute advantage estimate to reduce variance\n",
    "            future_rewards = compute_future_rewards(rewards,gamma)\n",
    "            future_rewards = torch.from_numpy(future_rewards).double().to(device)\n",
    "            # b = future_rewards.mean(axis=0)\n",
    "            # A = (future_rewards - b)/future_rewards.std(axis=0)\n",
    "            # A = torch.from_numpy(A).double().to(device)\n",
    "            \n",
    "            A = future_rewards-value_estimate\n",
    "            \n",
    "            # compute loss and applying gradient\n",
    "            loss = torch.sum(-log_probs*A)/(num_trajectories*horizon)\n",
    "            \n",
    "            undiscounted_future_rewards = compute_future_rewards(rewards,gamma=1.0)\n",
    "            undiscounted_future_rewards = torch.from_numpy(undiscounted_future_rewards).double().to(device)\n",
    "            loss += torch.sum((undiscounted_future_rewards-value_estimate)**2)/(num_trajectories*horizon)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            trajectory_total_rewards = rewards.sum(axis=1)\n",
    "            # update scores and score_window\n",
    "            scores.append(np.mean(trajectory_total_rewards))\n",
    "            scores_window.append(np.mean(trajectory_total_rewards))\n",
    "            \n",
    "            #printing progress\n",
    "            if opt_step % PRINT_EVERY == 0:\n",
    "                print (\"Episode: {} ... Avg reward: {:.2f}\".format(opt_step,np.mean(scores_window)))\n",
    "                # save the policy\n",
    "                torch.save(agent.policy, 'REINFORCE-Pendulum.policy')\n",
    "            \n",
    "            if np.mean(scores_window)>= -250.0:\n",
    "                print (\"Environment solved in {} optimization steps! ... Avg reward : {:.2f}\".format(opt_step-100,\n",
    "                                                                                          np.mean(scores_window)))\n",
    "                # save the policy\n",
    "                torch.save(agent.policy, 'REINFORCE-Pendulum.policy')\n",
    "                break\n",
    "                \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-99:\n",
      "Process Process-102:\n",
      "Process Process-97:\n",
      "Process Process-98:\n",
      "Process Process-104:\n",
      "Process Process-100:\n",
      "Process Process-101:\n",
      "Process Process-103:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 109, in worker\n",
      "    remote.send((ob, reward, done, info))\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 109, in worker\n",
      "    remote.send((ob, reward, done, info))\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 109, in worker\n",
      "    remote.send((ob, reward, done, info))\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 109, in worker\n",
      "    remote.send((ob, reward, done, info))\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 106, in worker\n",
      "    ob, reward, done, info = env.step(data)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/jsingh/learning/sys/REINFORCE/parallelEnv.py\", line 106, in worker\n",
      "    ob, reward, done, info = env.step(data)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 204, in send\n",
      "    self._check_closed()\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/jsingh/learning/sys/gym/gym/wrappers/time_limit.py\", line 31, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/Users/jsingh/learning/sys/gym/gym/wrappers/time_limit.py\", line 31, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 135, in _check_closed\n",
      "    if self._handle is None:\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/jsingh/learning/sys/gym/gym/envs/classic_control/pendulum.py\", line 41, in step\n",
      "    newthdot = thdot + (-3*g/(2*l) * np.sin(th + np.pi) + 3./(m*l**2)*u) * dt\n",
      "  File \"/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/jsingh/learning/sys/gym/gym/envs/classic_control/pendulum.py\", line 39, in step\n",
      "    costs = angle_normalize(th)**2 + .1*thdot**2 + .001*(u**2)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# lets define and train our agent\n",
    "agent = Agent(env_name='Pendulum-v0',learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsingh/anaconda3/envs/cv3/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-54919fe97eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_opt_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPRINT_EVERY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-5bb096160044>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_opt_steps, num_trajectories, horizon, gamma, PRINT_EVERY)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;31m# update state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/sys/REINFORCE/parallelEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/sys/REINFORCE/parallelEnv.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/sys/REINFORCE/parallelEnv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = agent.train(max_opt_steps=20000,horizon=200,gamma=0.98,PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reward curve over episodes\n",
    "plt.figure()\n",
    "plt.plot(scores)\n",
    "plt.xlabel('Episode #')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Watch the smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell to load the trained policy for Pendulum-v0\n",
    "# load policy\n",
    "policy =  torch.load('REINFORCE-Pendulum.policy',map_location='cpu')\n",
    "agent = Agent(env_name='Pendulum-v0')\n",
    "agent.policy = policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.figure(dpi = 72)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "total_reward = 0\n",
    "state = env.reset()\n",
    "for t in range(2000):\n",
    "    action, _ = agent.policy.act(state)\n",
    "    frames.append(env.render(mode='rgb_array')) \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state=next_state\n",
    "    total_reward+= reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print (\"Total reward:\",total_reward)\n",
    "env.close()\n",
    "animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(10,[3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 8, 9],\n",
       "        [5, 6, 6],\n",
       "        [4, 9, 5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 8, 9],\n",
       "       [6, 9, 5],\n",
       "       [3, 3, 7]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randint(0,10,[3,3])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  0,  0],\n",
       "        [-1, -3,  1],\n",
       "        [ 1,  6, -2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - torch.from_numpy(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([3,3],dtype=torch.double,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
